# Tech Spec: Push-Based Alerts (Phase 3)

## Purpose

Instead of requiring the AI to poll multiple tools to detect problems, the server proactively attaches an `_alerts` array to every `observe` response. Alerts surface regressions, anomalies, noise triggers, and CI/CD results that occurred since the AI's last call. The AI gets a situation briefing with every observation instead of needing separate tool calls.

## How It Works

### Alert Accumulation

The server maintains an alert buffer on `ToolHandler`. Alerts are generated by background checks and incoming webhooks. When any `observe` tool call is processed, accumulated alerts are attached to the response and the buffer is drained.

The alert buffer is a slice of `Alert` structs, capped at 50 entries. When full, oldest alerts are evicted (FIFO). Alerts are accumulated between `observe` calls — if the AI hasn't called `observe` in a while, it gets all pending alerts on the next call.

### Alert Structure

Each alert has:

- `severity`: one of `"info"`, `"warning"`, `"error"`
- `category`: one of `"regression"`, `"anomaly"`, `"ci"`, `"noise"`, `"threshold"`
- `title`: short human-readable summary (one line)
- `detail`: longer explanation with data points
- `timestamp`: ISO 8601 when the alert was generated
- `source`: what generated it (e.g., `"performance_monitor"`, `"ci_webhook"`, `"noise_detector"`)

### Alert Sources

**Performance regressions** (category: `"regression"`):
When a new performance snapshot arrives and the server detects a regression against the baseline (FCP/LCP/CLS/INP exceeding thresholds), an alert is generated. The threshold is 20% degradation from baseline for timing metrics, or 0.1 increase for CLS.

**Anomaly detection** (category: `"anomaly"`):
When error frequency spikes (more than 3x the rolling average in a 10-second window), an alert is generated. The rolling average is computed from the last 60 seconds of error entries.

**CI/CD results** (category: `"ci"`):
When the CI webhook receives a result, it generates an alert with the build status, test results summary, and any failure details.

**Noise rule triggers** (category: `"noise"`):
When auto-detection runs and finds new noise patterns, an informational alert is generated so the AI knows the signal-to-noise ratio improved.

**Threshold breaches** (category: `"threshold"`):
Memory pressure transitions (normal→soft, soft→hard) and circuit breaker state changes generate alerts.

### Response Format

The `observe` response gains an additional content block when alerts exist. The first content block remains the tool's normal output. A second content block with type `"text"` is appended containing the alerts as JSON:

```json
{
  "content": [
    {"type": "text", "text": "<normal observe output>"},
    {"type": "text", "text": "--- ALERTS (3) ---\n<alerts JSON array>"}
  ]
}
```

The alerts block is only appended when there are pending alerts. When no alerts exist, the response is identical to the current behavior (single content block).

### Situation Synthesis

Rather than dumping raw alerts, the server performs basic triage:

1. **Deduplication**: If the same regression is detected multiple times (e.g., repeated page loads all showing the same LCP regression), only one alert is generated with a count field.
2. **Priority ordering**: Alerts are sorted by severity (error > warning > info) then by timestamp (newest first).
3. **Correlation**: If a performance regression and an error spike occur within the same 5-second window, they're grouped into a single compound alert with both details.
4. **Summary prefix**: When more than 3 alerts exist, the alerts block starts with a one-line summary: "3 alerts: 1 regression, 1 anomaly, 1 CI failure"

### CI/CD Webhook Receiver

A new HTTP endpoint: `POST /ci-result`

Request body (JSON):
```json
{
  "status": "success" | "failure" | "error",
  "source": "github-actions" | "gitlab-ci" | "custom",
  "ref": "main",
  "commit": "abc123",
  "summary": "12 tests passed, 2 failed",
  "failures": [
    {"name": "test_login", "message": "Expected 200, got 401"}
  ],
  "url": "https://github.com/org/repo/actions/runs/123",
  "duration_ms": 45000
}
```

Response: `{"ok": true}` on success, appropriate error on failure.

The webhook has no authentication (localhost-only tool). Request body is limited to 1MB via `http.MaxBytesReader`. The endpoint stores the CI result and generates an alert.

Only the most recent 10 CI results are kept. Older results are evicted.

### Alert Generation Timing

- **Performance regressions**: Generated synchronously when `AddPerformanceSnapshot` is called on Capture (the existing baseline comparison runs at ingest time).
- **Anomaly detection**: Checked every time a new log entry is added. Uses a simple sliding window counter — no background goroutine needed.
- **CI results**: Generated synchronously on webhook receipt.
- **Threshold breaches**: Generated when memory/circuit state transitions occur (already detected in existing code).
- **Noise detection**: Generated when `auto_detect` noise action runs.

### Concurrency

The alert buffer has its own mutex (`alertMu sync.Mutex`) on `ToolHandler`. Alert generation acquires `alertMu` to append. Alert drain (in `observe` dispatch) acquires `alertMu` to read and clear. This is separate from `server.mu` and `capture.mu` to avoid lock ordering issues.

## Edge Cases

- If the alert buffer is full (50 entries) and a new alert arrives, the oldest alert is evicted. An additional meta-alert is NOT generated (to avoid infinite loops).
- If no `observe` call has ever been made, alerts still accumulate (up to the 50-entry cap).
- The CI webhook is idempotent — posting the same commit+status twice updates rather than duplicates.
- Empty `failures` array in CI webhook is valid (success case).
- Correlation window (5 seconds) uses the alert timestamps, not wall clock at drain time.

## Performance Constraints

- Alert generation must be O(1) — just append to a slice.
- Alert drain is O(n) where n ≤ 50 — copy and clear under mutex.
- CI webhook response must be under 5ms.
- Anomaly detection (error frequency check) is O(1) — maintains a counter, not a scan.

## Test Scenarios

1. Fresh server, no alerts: `observe` response has single content block (no alerts section).
2. Add performance regression: next `observe` call includes regression alert.
3. Multiple alerts accumulate: all are returned on next `observe`, then buffer is empty.
4. Alert cap (50): oldest evicted when buffer full.
5. CI webhook: POST valid result → alert generated → next `observe` includes CI alert.
6. CI webhook: POST invalid body → 400 error, no alert.
7. CI webhook: POST same commit twice → updates existing, no duplicate alert.
8. Situation synthesis: error + regression in same window → correlated compound alert.
9. Alert priority ordering: errors before warnings before info.
10. Deduplication: same regression repeated → single alert with count.
11. Summary prefix: 4+ alerts → starts with summary line.
12. Anomaly detection: error spike (>3x average) → anomaly alert generated.
13. Threshold breach: memory pressure transition → alert generated.
14. Noise auto-detect: new patterns found → info alert generated.
