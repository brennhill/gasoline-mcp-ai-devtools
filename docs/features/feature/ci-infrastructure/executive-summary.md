---
feature: Gasoline CI Infrastructure
status: proposed
tier: v6.0 Wave 1 (Core Thesis)
---

# Executive Summary: Gasoline CI Infrastructure

## What It Does (In 30 Seconds)

**Gasoline CI Infrastructure** captures browser state (DOM, network calls, logs) when tests fail in CI, then gives AI agents the ability to autonomously diagnose failures and repair tests **in the same container**, with automatic verification.

**Before:** Test fails in GitHub Actions → Engineer spends 45 minutes reproducing locally → Manually diagnoses → Fixes locally → Re-runs in CI to confirm

**After:** Test fails in GitHub Actions → AI sees snapshot of browser state → Diagnoses in 30 seconds → Applies fix to test → Re-runs in CI automatically → Verifies pass → Suggests fix to engineer

**Key Difference:** Instead of "error text only," AI has the same observability in CI that it has in local development.

---

## Who Uses It & When

### **Primary Users: Engineering Teams**

**Engineering Manager or DevOps Lead**
- **Problem:** "Our engineers spend 30% of time debugging CI failures, not shipping features"
- **When they use it:** When a test fails in CI pipeline (GitHub Actions, GitLab CI, CircleCI)
- **What they see:** Automatic GitHub comment appears: "Root cause: API timeout. Fix applied. Verified passing."
- **Value:** Reclaim 30% of engineering time; faster releases

**Example workflow:**
```
Engineer pushes code
  ↓
GitHub Actions runs tests
  ↓
Test fails: "Cannot find element"
  ↓
[AUTOMATIC] Gasoline captures snapshot (DOM, network, logs)
  ↓
[AUTOMATIC] AI diagnoses: "Selector is stale; use data-testid instead"
  ↓
[AUTOMATIC] AI applies fix + re-runs test
  ↓
[AUTOMATIC] GitHub comment appears with proof
  ↓
Engineer reviews comment, merges if satisfied
```

---

### **Secondary Users: AI Coding Agent Companies**

**Anthropic, Cursor, GitHub Copilot, etc.**
- **Problem:** "Our AI agents work great locally but can't repair tests in CI"
- **When they use it:** When proposing fixes to test failures
- **What they see:** Integration with Gasoline means fixes are verified before suggesting
- **Value:** End-to-end autonomy (diagnose + repair + verify in CI)

---

### **Tertiary Users: Enterprise Compliance Teams**

**CISO, Compliance Officer, Audit Team**
- **Problem:** "We want AI to modify code, but need proof it works before merge"
- **When they use it:** When reviewing AI-proposed code changes in PRs
- **What they see:** Full audit trail (failure → snapshot → fix → re-run proof)
- **Value:** Compliance-ready AI; "AI-verified" changes are safer

---

## How It Fits Into Workflows

### **Workflow 1: Test Debugging (Today)**

```
STEP 1: Test Fails in CI (5 min)
┌─────────────────────────────┐
│ GitHub Actions logs:        │
│ ❌ Test failed at line 45   │
│ Expected: "Success"         │
│ Got: "Loading"              │
└─────────────────────────────┘

STEP 2: Engineer Debugs Locally (30-45 min)
┌─────────────────────────────┐
│ • Clone repo                │
│ • Install dependencies      │
│ • npm test                  │
│ • Browser opens             │
│ • Manual inspection         │
│ • "Ah! DOM shows spinner"   │
│ • Checks network tab        │
│ • "POST /api/order timeout" │
└─────────────────────────────┘

STEP 3: Engineer Proposes Fix (10 min)
┌─────────────────────────────┐
│ • Increase timeout to 20s    │
│ • npm test (verify locally) │
│ • git commit + push         │
└─────────────────────────────┘

STEP 4: CI Re-Runs (5 min)
┌─────────────────────────────┐
│ GitHub Actions: ✅ PASS     │
└─────────────────────────────┘

TOTAL TIME: 50-60 minutes per failure
```

### **Workflow 2: Test Debugging (With CI Infrastructure)**

```
STEP 1: Test Fails in CI (5 min)
┌─────────────────────────────┐
│ GitHub Actions logs:        │
│ ❌ Test failed at line 45   │
│ Expected: "Success"         │
│ Got: "Loading"              │
│                             │
│ [Gasoline auto-captures]    │
│ • DOM snapshot              │
│ • Network calls             │
│ • Console logs              │
│ • Request/response bodies   │
└─────────────────────────────┘

STEP 2: AI Diagnoses (1 min)
┌─────────────────────────────┐
│ [AI analysis of snapshot]   │
│ "API timeout detected:      │
│  POST /api/order took 15s   │
│  but test timeout is 5s"    │
│                             │
│ Confidence: 95%             │
└─────────────────────────────┘

STEP 3: AI Repairs & Verifies (2 min)
┌─────────────────────────────┐
│ [AI action in CI]           │
│ • Change timeout to 20s     │
│ • Mock slow endpoint        │
│ • Re-run test in CI         │
│ • ✅ Test passes            │
│ • No regressions detected   │
└─────────────────────────────┘

STEP 4: Engineer Reviews (5 min)
┌─────────────────────────────┐
│ GitHub PR comment:          │
│ ✅ Fix Applied & Verified   │
│                             │
│ ROOT CAUSE:                 │
│ API timeout (15s) > test    │
│ timeout (5s)                │
│                             │
│ FIX: Increase timeout 5→20s │
│                             │
│ VERIFICATION:               │
│ ✅ Re-run passed            │
│ ✅ No regressions           │
│                             │
│ Ready to merge?             │
└─────────────────────────────┘

TOTAL TIME: 13 minutes per failure (4x faster)
```

---

## Concrete Features & Capabilities

### **Feature 1: Test Snapshots**

**What it does:** Captures full browser state at a specific test moment

```javascript
test('checkout', async ({ page }) => {
  await gasoline.snapshot('before-purchase');  // ← Capture state
  
  await page.click('[data-testid="purchase"]');
  await expect(page).toContainText('Success');  // ← If fails, AI can restore to snapshot
});
```

**Why it matters:** AI can say "what was the DOM/network state before the test failed?" without reproducing locally

---

### **Feature 2: Test Isolation (Test Boundaries)**

**What it does:** Marks "this log is from my test" vs "background noise"

```javascript
test('user login', async ({ page, gasoline }) => {
  await gasoline.testBoundary('login-test');  // ← Mark test-specific logs
  
  await page.fill('[name="email"]', 'user@example.com');
  await page.click('[data-testid="login"]');
  
  // Logs captured:
  // ✓ POST /api/login (test-specific)
  // ✓ "User authenticated" (test-specific)
  // ✗ GET /analytics (ignored, not test-specific)
});
```

**Why it matters:** AI sees only relevant logs (80% less noise) → diagnoses 3x faster

---

### **Feature 3: Network Mocking**

**What it does:** AI tells Gasoline "make this API return this response"

```javascript
// AI applies this during diagnosis:
await gasoline.configure({
  action: 'mock',
  endpoint: '/api/payment',
  response: { statusCode: 402, body: { error: 'Card declined' } }
});

// Re-runs test with mocked response
// Can verify: Does UI handle 402 error correctly?
```

**Why it matters:** AI can test error paths and edge cases without touching actual backend

---

### **Feature 4: Playwright Fixtures**

**What it does:** Makes Gasoline APIs available in test setup/teardown

```typescript
import { test as base } from '@playwright/test';
import { Gasoline } from './gasoline';

export const test = base.extend({
  gasoline: async ({ page }, use) => {
    const g = new Gasoline();
    await g.initialize();
    await use(g);
    await g.cleanup();
  }
});

// Now in tests:
test('my test', async ({ page, gasoline }) => {
  // Full Gasoline API available
  await gasoline.snapshot('state1');
  await gasoline.configure({ action: 'mock', ... });
});
```

**Why it matters:** Zero friction to adopt; copy-paste into existing tests

---

### **Feature 5: Async Execution**

**What it does:** Prevents MCP server from hanging when AI re-runs tests (which can take 30+ seconds)

```javascript
// Old (hangs server):
await gasoline.rerunTest('checkout.spec.ts');  // Server blocked for 30 seconds

// New (async-safe):
const result = await gasoline.async(() => {
  return gasoline.rerunTest('checkout.spec.ts');  // Server still responsive
});
```

**Why it matters:** Multiple tests can re-run concurrently; MCP server never blocks

---

### **Feature 6: CI Output Formats**

**What it does:** Generates industry-standard formats for GitHub/GitLab to parse

**HAR (HTTP Archive):**
- All network calls + responses
- Performance metrics
- Can view in browser DevTools

**SARIF (Static Analysis Results):**
- Code violations + exact line numbers
- GitHub shows inline PR comments
- Compliance audit trail

**Screenshots:**
- Visual state at failure moment
- Attached to GitHub/GitLab artifacts
- Engineers see what AI saw

---

## Example: Real-World Scenario

### **Scenario: "Tests Pass Locally, Fail in CI"**

**The Problem:**
```
Local machine:
  ✅ npm test passes (PostgreSQL 12, node 18)

CI container:
  ❌ npm test fails (PostgreSQL 14, node 20)
  
  Error: Cannot find column "user_metadata"
  (Schema changed between PG versions)
  
Today: Engineer must debug in CI environment (120+ minutes)
```

**With CI Infrastructure:**

```
Step 1: Snapshot captured
┌────────────────────────────────┐
│ Database error in logs:        │
│ "Column 'user_metadata' does   │
│  not exist in PostgreSQL 14"   │
│                                │
│ Network trace shows:           │
│ SELECT * FROM users (failed)   │
└────────────────────────────────┘

Step 2: AI diagnoses (2 min)
┌────────────────────────────────┐
│ "Schema mismatch detected.     │
│  Test mocks PG12 schema,      │
│  but CI has PG14 schema.      │
│  Column 'user_metadata' moved  │
│  to 'user_info' in PG14."     │
│                                │
│ Confidence: 92%                │
└────────────────────────────────┘

Step 3: AI repairs (3 min)
┌────────────────────────────────┐
│ Updates mock:                  │
│ BEFORE: users (id, name, ...)  │
│ AFTER:  users (id, name, ...)  │
│         + user_info (metadata) │
│                                │
│ Re-runs test: ✅ PASS          │
│ Regression check: ✅ PASS      │
└────────────────────────────────┘

Step 4: Engineer reviews (2 min)
┌────────────────────────────────┐
│ GitHub comment:                │
│ ✅ Fix Applied                 │
│                                │
│ ROOT CAUSE:                    │
│ PostgreSQL schema version      │
│ mismatch (12 vs 14)            │
│                                │
│ FIX:                           │
│ Updated mock to reflect PG14   │
│ schema (user_info table)       │
│                                │
│ Ready to merge!                │
└────────────────────────────────┘

TOTAL TIME: 7 minutes
(vs 120+ minutes manual debugging in CI)
```

---

## Business Impact Summary

### **For Engineering Teams**

| Metric | Before | After | Savings |
|--------|--------|-------|---------|
| Time per failure | 45 min | 5 min | **40 min/failure** |
| Failures/day | 20 | 20 | (same) |
| Wasted time/day | 900 min (15h) | 100 min (1.7h) | **13.3 hours/day** |
| Annual waste | 3,750 hours ($750k) | 417 hours ($83k) | **$667k saved/year** |

**For 100-person team:**
- **Reclaim 30% of engineering capacity** (engineers stop debugging, start shipping)
- **Faster releases** (red builds green in minutes, not hours)
- **Reduced context-switching** (engineers notified of fix, not blocked waiting)

---

### **For AI Companies**

- **End-to-end autonomy:** Diagnose + repair + verify (all in CI)
- **Competitive moat:** Only solution that works in pipelines at scale
- **Enterprise unlock:** "AI-verified" fixes enable corporate adoption

---

### **For Enterprises**

- **Compliance-ready:** Full audit trail (failure → snapshot → fix → verify)
- **Risk reduction:** AI changes verified before merge
- **Security:** Snapshot context never leaves CI environment

---

## When Engineers Use It (Timeline)

```
Developer workflow:
  Morning: Pushes feature branch
    ↓
  GitHub Actions kicks off tests
    ↓
  [5 min] One test fails in checkout.spec.ts
    ↓
  [30 sec] Gasoline snapshot captured
    ↓
  [1 min] AI diagnoses: "Selector timeout; increase to 10s"
    ↓
  [2 min] AI applies + verifies fix in CI
    ↓
  [2 min] GitHub comment appears with evidence
    ↓
  Developer reviews comment
    ↓
  Developer approves + merges
    ↓
  [10 min] Full CI passes, deployed to staging
    ↓
  10:30 AM: Back to coding (lost only 10 minutes, not 50)
```

---

## Key Differentiator

### What Makes This Different

**GitHub Actions (native):**
- Just logs error text
- No root cause
- No repair capability

**Playwright Trace Viewer (native):**
- Records full trace (good)
- Requires manual inspection (tedious)
- No autonomous diagnosis
- No repair capability

**Manual Debugging (current reality):**
- Engineer reproduces locally
- Takes 30-45 minutes
- Ties up engineer

**Gasoline CI Infrastructure (new):**
- ✅ Captures full browser state automatically
- ✅ AI diagnoses autonomously
- ✅ AI repairs + verifies automatically
- ✅ Takes 5 minutes, engineers stay unblocked

**The advantage:** Only solution that brings **autonomous diagnosis + repair to CI pipelines**

---

## Launch & Adoption Path

### **Phase 1: Developer Mindshare (Months 1-3)**
- Free tier: 100 snapshots/month
- GitHub Actions integration
- Target: Build community adoption

### **Phase 2: Enterprise Sales (Months 4-12)**
- Paid tier: $200-500/month per team
- Advanced features: SARIF, HAR, artifact retention
- Compliance positioning
- Target: Mid-market teams with AI agent adoption

### **Phase 3: Platform Lock-in (Year 2+)**
- Become default CI diagnostics layer
- Native GitHub/GitLab integrations
- Target: Mainstream adoption

---

## Bottom Line

**Gasoline CI Infrastructure** turns CI test failures from a 50-minute drain into a 5-minute autonomous process. It's the difference between "tests fail, engineer debugs" and "tests fail, AI diagnoses, engineer reviews fix."

**ROI:** Saves $667k/year per 100-person engineering team. Payback period: <1 day.

